{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c8dfaa-b6d5-44ce-b31a-e48a06347f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gymnasium\n",
    "# !apt-get update\n",
    "# !apt-get install ffmpeg libsm6 libxext6  -y\n",
    "# !apt install -y libgl1-mesa-glx\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "769d1335-d30e-4339-8d64-d35076b04f17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#adapted from https://www.kaggle.com/code/danieldreher/vanilla-dqn-cartpole-tensorflow-2-3/notebook \n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rand\n",
    "import cv2\n",
    "import collections\n",
    "from collections import deque\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a12ad070-b7e5-4d63-ade0-b82da1e951af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ProcessFrame84(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Downsamples image to 84x84\n",
    "    Greyscales image\n",
    "\n",
    "    Returns numpy array\n",
    "    \"\"\"\n",
    "    def __init__(self, env=None):\n",
    "        super(ProcessFrame84, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        return ProcessFrame84.process(obs)\n",
    "\n",
    "    @staticmethod\n",
    "    def process(frame):\n",
    "        if frame.size == 210 * 160 * 3:\n",
    "            img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n",
    "        else:\n",
    "            assert False, \"Unknown resolution.\"\n",
    "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
    "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
    "        x_t = resized_screen[18:102, :]\n",
    "        x_t = np.reshape(x_t, [84, 84, 1])\n",
    "        return x_t.astype(np.uint8)\n",
    "\n",
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    \"\"\"Normalize pixel values in frame --> 0 to 1\"\"\"\n",
    "    def observation(self, obs):\n",
    "        return np.array(obs).astype(np.float32) / 255.0\n",
    "\n",
    "    \n",
    "def make_env(env, obs_type=\"grayscale\"):\n",
    "    env = ProcessFrame84(env)\n",
    "    return ScaledFloatFrame(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef65d081-8715-47cf-972d-873003fa529d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#the replay buffer contains episode transitions in the\n",
    "#  order the episode is generated\n",
    "#The Python collections deque has a pointer to the next\n",
    "#  and previous element for all elements\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.replay_memory = deque(maxlen=buffer_size)    \n",
    "\n",
    "    #add one transition to the replay buffer\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.replay_memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    #take a random sample from the replay buffer for training\n",
    "    def sample(self, batch_size):\n",
    "        if batch_size <= len(self.replay_memory):\n",
    "            return rand.sample(self.replay_memory, batch_size)\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "    #Python magic method to enable len to be used on a replay buffer object\n",
    "    def __len__(self):\n",
    "        return len(self.replay_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0adef140-dbc5-4d23-af0d-ca6704ba62b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Class to implement an epsilon decay schedule\n",
    "#epsilon starts high and then reduces by a decay factor\n",
    "#The decay factor can be changed according to how many training iterations\n",
    "#  are completed\n",
    "class EpsilonSchedule():\n",
    "    def __init__(self, final_epsilon=0.1, pre_train_steps=10, final_exploration_step=100):\n",
    "        self.pre_train_steps = pre_train_steps\n",
    "        self.final_exploration_step = final_exploration_step\n",
    "        self.final_epsilon = final_epsilon\n",
    "        self.decay_factor = self.pre_train_steps/self.final_exploration_step\n",
    "        self.epsilon = 1\n",
    "    \n",
    "    def value(self, t):\n",
    "        if t > self.pre_train_steps:\n",
    "            self.decay_factor = (t - self.pre_train_steps)/self.final_exploration_step\n",
    "            self.epsilon = 1-self.decay_factor\n",
    "            self.epsilon = max(self.final_epsilon, self.epsilon)\n",
    "            return self.epsilon\n",
    "        else:\n",
    "            return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb4acae7-3e05-4d56-b800-9cbb217c418a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#define the neural network model using keras\n",
    "class DQN(keras.Model):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        # self.input_layer = keras.layers.InputLayer(input_shape=input_shape)\n",
    "        # self.hidden_layers = []\n",
    "        \n",
    "                        # tf.keras.layers.InputLayer(input_shape=input_shape),\n",
    "        print(input_shape)\n",
    "        self.net = tf.keras.Sequential(\n",
    "            [\n",
    "                # tf.keras.layers.Input(shape=input_shape), #Input shape will be a 210 x 160 rgb image (change 3 to 1 and double check the dimensions of the input image if grayscale is used\n",
    "                keras.layers.InputLayer(input_shape=input_shape),\n",
    "                tf.keras.layers.Conv2D(32, 10, strides = 2, activation = \"gelu\", padding = \"same\"), #32 10 x 10 filters with a stride length of 2 and a padding of 0s around the edges of the image\n",
    "                tf.keras.layers.Conv2D(32, 10, activation = \"gelu\"), #32 10 x 10 filters with a stride length of 1\n",
    "                tf.keras.layers.Conv2D(64, 10, activation = \"gelu\"), #64 10 x 10 filters with a stride length of 1\n",
    "                tf.keras.layers.Conv2D(64, 5, activation = \"gelu\"), #64 5 x 5 filters with a stride length of 1\n",
    "                tf.keras.layers.MaxPooling2D(5), #Max Pooling using a 5 x 5 filter\n",
    "                #Max pooling takes the highest value in the filter an makes it the value of a smaller \"image,\" unlike the convolutional layers, the filter does not overlap itself\n",
    "                tf.keras.layers.Flatten(), #Converts the data up to this point into a 1D array\n",
    "                tf.keras.layers.Dense(256, activation = \"gelu\"), #Single regression layer for a small boost to feature extraction\n",
    "                tf.keras.layers.Dense(num_actions, activation = \"softmax\") #Determines the class\n",
    "                #Softmax activation will bind the results to a range of [0, 1], with the sum of all nodes equaling 1\n",
    "                #This allows the final layer to present a probability of each action\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # self.hidden_layers.append(keras.layers.Dense(64, activation='relu'))\n",
    "        # self.hidden_layers.append(keras.layers.Dense(32, activation='relu'))\n",
    "        # self.output_layer = keras.layers.Dense(units=num_actions, activation='linear')\n",
    "\n",
    "    #forward pass of the model\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        # z = self.input_layer(inputs)\n",
    "        # for l in self.hidden_layers:\n",
    "        #     z = l(z)\n",
    "        # q_vals = self.output_layer(z)\n",
    "        q_vals = self.net(inputs)\n",
    "        return q_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db781b0e-3d8b-4602-9005-c8915431f788",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, gamma=0.9, batch_size=64, lr=0.001,\n",
    "                 max_episodes = 500, max_steps_per_episode=2000,\n",
    "                 steps_until_sync=20, choose_action_frequency=1,\n",
    "                 pre_train_steps = 1, final_exploration_step = 100):\n",
    "        \n",
    "        self.env = env\n",
    "        self.input_shape = list(env.observation_space.shape)\n",
    "        self.num_actions = env.action_space.n\n",
    "        # dqn is used to predict Q-values to decide which action to take\n",
    "        self.dqn = DQN(self.input_shape, self.num_actions)\n",
    "        #build is used for subclassed models and takes the input shape as an argument\n",
    "        #build builds the model\n",
    "        self.dqn.build(tf.TensorShape([None, *self.input_shape]))\n",
    "        \n",
    "        # second DQN to predit the future reward of Q(s',a)\n",
    "        # dqn_target is used to predict the future reward\n",
    "        self.dqn_target = DQN(self.input_shape, self.num_actions)\n",
    "        self.dqn_target.build(tf.TensorShape([None, *self.input_shape]))\n",
    "\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        #stochastic gradient method, lr is learning rate\n",
    "        self.optimizer = tf.optimizers.legacy.Adam(lr)\n",
    "        #discount factor\n",
    "        self.gamma = gamma\n",
    "        #to fill up the replay buffer\n",
    "        self.pre_train_steps = pre_train_steps\n",
    "        self.final_exploration_step = final_exploration_step\n",
    "        self.replay_buffer = ReplayBuffer(max_episodes*max_steps_per_episode)\n",
    "        self.epsilon_schedule = EpsilonSchedule(final_epsilon=0.1, \n",
    "                pre_train_steps=self.pre_train_steps,\n",
    "                final_exploration_step=self.final_exploration_step)\n",
    "        #steps until the target dqn is updated with the current dqn\n",
    "        self.steps_until_sync = steps_until_sync\n",
    "        #choose a new action every action frequency steps\n",
    "        self.choose_action_frequency = choose_action_frequency\n",
    "        self.max_episodes = max_episodes\n",
    "        self.max_steps_per_episode = max_steps_per_episode\n",
    "        #loss function of mean squared error\n",
    "        self.loss_function = tf.keras.losses.MSE\n",
    "        self.episode_reward_history = []\n",
    "\n",
    "    #predict the q values\n",
    "    def predict_q(self, inputs):\n",
    "        return self.dqn(inputs)\n",
    "\n",
    "    def get_action(self, states, epsilon):\n",
    "        if np.random.random() < epsilon:\n",
    "            # explore\n",
    "            return np.random.choice(self.num_actions)\n",
    "        else:\n",
    "            # exploit\n",
    "            return np.argmax(self.predict_q(np.expand_dims(states, axis=0))[0])\n",
    "\n",
    "    #copy dqn into dqn_target\n",
    "    def update_target_network(self):\n",
    "        self.dqn_target.set_weights(self.dqn.get_weights())\n",
    "\n",
    "    #take a training step\n",
    "    def train_step(self):\n",
    "        #take a random sample from the replay buffer\n",
    "        mini_batch = self.replay_buffer.sample(self.batch_size)\n",
    "        #unzip the random sample into separate vectors\n",
    "        observations_batch, action_batch, reward_batch, next_observations_batch, done_batch = map(np.array,zip(*mini_batch))\n",
    "        #record operations for automatic differentiation\n",
    "        with tf.GradientTape() as tape:\n",
    "            #watch the trainable variables\n",
    "            dqn_variables = self.dqn.trainable_variables\n",
    "            tape.watch(dqn_variables)\n",
    "            #compute the rewards of the next state\n",
    "            future_rewards = self.dqn_target(tf.convert_to_tensor(next_observations_batch, dtype=tf.float32))\n",
    "            next_action = tf.argmax(future_rewards, axis=1)\n",
    "            #find the sum of elements across the columns of the tensor\n",
    "            #one_hot is used to mask out any q values that are unneeded\n",
    "            target_q = tf.reduce_sum(tf.one_hot(next_action, self.num_actions) * future_rewards, axis=1)\n",
    "            #update the future rewards eliminating any states that were terminal states\n",
    "            target_q = (1 - done_batch) * self.gamma * target_q + reward_batch\n",
    "            #do the same for the current state\n",
    "            predicted_q = self.dqn(tf.convert_to_tensor(observations_batch, dtype=tf.float32))\n",
    "            predicted_q = tf.reduce_sum(tf.one_hot(action_batch, self.num_actions) * predicted_q, axis=1)\n",
    "            #find the loss between the tartget and the predicted\n",
    "            loss = self.loss_function(target_q, predicted_q)   \n",
    "        # Backpropagate the loss\n",
    "        gradients = tape.gradient(loss, dqn_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, dqn_variables))\n",
    "        #return the loss\n",
    "        return loss\n",
    "\n",
    "    def train(self):\n",
    "        episode = 0\n",
    "        total_step = 0\n",
    "        episode_step = 0\n",
    "        state, info = self.env.reset()\n",
    "        loss = 0\n",
    "        last_hundred_rewards = deque(maxlen=100)\n",
    "\n",
    "        while episode < self.max_episodes:\n",
    "            current_state, info = self.env.reset()\n",
    "            done = False\n",
    "            action = 0\n",
    "            episode_reward = 0\n",
    "            episode_step = 0\n",
    "            epsilon = self.epsilon_schedule.value(total_step)\n",
    "\n",
    "            while not done:\n",
    "                #control the number of times a new action is chosen\n",
    "                if total_step % self.choose_action_frequency == 0:\n",
    "                    if len(self.replay_buffer) > self.batch_size:\n",
    "                        action = self.get_action(current_state, epsilon)\n",
    "                    else:\n",
    "                        action = self.get_action(current_state, 1.0)\n",
    "\n",
    "                next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                self.replay_buffer.add(current_state, action, reward, next_state, done)\n",
    "                episode_reward += reward\n",
    "                \n",
    "                #train the dqn if enough data samples are available\n",
    "                if total_step > self.pre_train_steps and len(self.replay_buffer) > self.batch_size:\n",
    "                    loss = self.train_step()\n",
    "                #control how often the target dqn is updated in order to foster stability in the target q value\n",
    "                if total_step % self.steps_until_sync == 0:\n",
    "                    self.update_target_network()\n",
    "                                    \n",
    "                #end of step\n",
    "                total_step += 1\n",
    "                episode_step += 1\n",
    "                current_state = next_state\n",
    "                \n",
    "            # end of episode\n",
    "            self.episode_reward_history.append(episode_reward)\n",
    "            last_hundred_rewards.append(episode_reward)\n",
    "            mean_episode_reward = np.mean(last_hundred_rewards)\n",
    "            #show the average reward\n",
    "            if episode > -1:\n",
    "                print('\\n' + f'Episode {episode} (Step {total_step}) - Moving Avg Reward: {mean_episode_reward:.3f} Loss: {loss:.5f} Epsilon: {epsilon:.3f}')\n",
    "            else:\n",
    "                print(\"*\", end=\"\")\n",
    "            #stop training if the mean of the last 100 rewards is nearing 200\n",
    "            if mean_episode_reward >= 195:\n",
    "                print(f'Task solved after {episode} episodes! (Moving Avg Reward: {mean_episode_reward:.3f})')\n",
    "                return                \n",
    "            episode += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59c0a951-957f-4bc1-93aa-210e06386cd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(18)\n",
      "Action space size: 18\n",
      "Observation space shape: (84, 84, 1)\n",
      "EnvSpec(id='ALE/MarioBros-v5', entry_point='shimmy.atari_env:AtariEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'game': 'mario_bros', 'obs_type': 'rgb', 'repeat_action_probability': 0.25, 'full_action_space': False, 'frameskip': 4, 'max_num_frames_per_episode': 500}, namespace='ALE', name='MarioBros', version=5, additional_wrappers=(WrapperSpec(name='ProcessFrame84', entry_point='__main__:ProcessFrame84', kwargs=None), WrapperSpec(name='ScaledFloatFrame', entry_point='__main__:ScaledFloatFrame', kwargs=None)), vector_entry_point=None)\n"
     ]
    }
   ],
   "source": [
    "GAME = \"ALE/MarioBros-v5\"\n",
    "env = make_env(gym.make(GAME, max_num_frames_per_episode=500))\n",
    "print(\"Action space: {}\".format(env.action_space))\n",
    "print(\"Action space size: {}\".format(env.action_space.n))\n",
    "observation, info = env.reset()\n",
    "print(\"Observation space shape: {}\".format(observation.shape))\n",
    "print(env.spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00abe454-5b02-4020-a2fe-d505ed87ee85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[84, 84, 1]\n",
      "[84, 84, 1]\n",
      "\n",
      "Episode 0 (Step 47) - Moving Avg Reward: 0.000 Loss: 0.00000 Epsilon: 1.000\n",
      "\n",
      "Episode 1 (Step 94) - Moving Avg Reward: 0.000 Loss: 0.00006 Epsilon: 0.995\n",
      "\n",
      "Episode 2 (Step 141) - Moving Avg Reward: 0.000 Loss: 0.00008 Epsilon: 0.991\n",
      "\n",
      "Episode 3 (Step 188) - Moving Avg Reward: 0.000 Loss: 0.00005 Epsilon: 0.986\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m env \u001b[38;5;241m=\u001b[39m make_env(gym\u001b[38;5;241m.\u001b[39mmake(GAME, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, max_num_frames_per_episode\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m))\n\u001b[1;32m      4\u001b[0m agent \u001b[38;5;241m=\u001b[39m Agent(env, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.99\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0007\u001b[39m, max_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m      5\u001b[0m               steps_until_sync\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, choose_action_frequency\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      6\u001b[0m               pre_train_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, final_exploration_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10_000\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m agent\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      9\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[0;32mIn[22], line 122\u001b[0m, in \u001b[0;36mAgent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m#train the dqn if enough data samples are available\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_step \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_train_steps \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size:\n\u001b[0;32m--> 122\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_step()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m#control how often the target dqn is updated in order to foster stability in the target q value\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_until_sync \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[22], line 85\u001b[0m, in \u001b[0;36mAgent.train_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_function(target_q, predicted_q)   \n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Backpropagate the loss\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, dqn_variables)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(gradients, dqn_variables))\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m#return the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/backprop.py:1065\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1059\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1060\u001b[0m       composite_tensor_gradient\u001b[38;5;241m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[1;32m   1061\u001b[0m           output_gradients))\n\u001b[1;32m   1062\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x)\n\u001b[1;32m   1063\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m output_gradients]\n\u001b[0;32m-> 1065\u001b[0m flat_grad \u001b[38;5;241m=\u001b[39m imperative_grad\u001b[38;5;241m.\u001b[39mimperative_grad(\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape,\n\u001b[1;32m   1067\u001b[0m     flat_targets,\n\u001b[1;32m   1068\u001b[0m     flat_sources,\n\u001b[1;32m   1069\u001b[0m     output_gradients\u001b[38;5;241m=\u001b[39moutput_gradients,\n\u001b[1;32m   1070\u001b[0m     sources_raw\u001b[38;5;241m=\u001b[39mflat_sources_raw,\n\u001b[1;32m   1071\u001b[0m     unconnected_gradients\u001b[38;5;241m=\u001b[39munconnected_gradients)\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent:\n\u001b[1;32m   1074\u001b[0m   \u001b[38;5;66;03m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[1;32m   1075\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_watched_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape\u001b[38;5;241m.\u001b[39mwatched_variables()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown value for unconnected_gradients: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m unconnected_gradients)\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_TapeGradient(\n\u001b[1;32m     68\u001b[0m     tape\u001b[38;5;241m.\u001b[39m_tape,  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     target,\n\u001b[1;32m     70\u001b[0m     sources,\n\u001b[1;32m     71\u001b[0m     output_gradients,\n\u001b[1;32m     72\u001b[0m     sources_raw,\n\u001b[1;32m     73\u001b[0m     compat\u001b[38;5;241m.\u001b[39mas_str(unconnected_gradients\u001b[38;5;241m.\u001b[39mvalue))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:760\u001b[0m, in \u001b[0;36m_TapeGradientFunctions._wrap_backward_function.<locals>._backward_function_wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    758\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m input_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m backward_function_inputs:\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 760\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m backward\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    761\u001b[0m     processed_args, remapped_captures)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1260\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1262\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1263\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1264\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mflat_call(args)\n\u001b[1;32m   1265\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1266\u001b[0m     args,\n\u001b[1;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1268\u001b[0m     executing_eagerly)\n\u001b[1;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflat_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    216\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    218\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    251\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 252\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[1;32m    256\u001b[0m     )\n\u001b[1;32m    257\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    262\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1479\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[1;32m   1480\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1481\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   1482\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[1;32m   1483\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[1;32m   1484\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1485\u001b[0m   )\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1487\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1488\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1489\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1493\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1494\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     55\u001b[0m       tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[1;32m     56\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m     57\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[1;32m     59\u001b[0m   ]\n\u001b[0;32m---> 60\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     61\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     63\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Train the agent\n",
    "env = make_env(gym.make(GAME, mode=4, max_num_frames_per_episode=200))\n",
    "\n",
    "agent = Agent(env, gamma=0.99, batch_size=64, lr=0.0007, max_episodes=1000,\n",
    "              steps_until_sync=500, choose_action_frequency=1,\n",
    "              pre_train_steps = 1, final_exploration_step = 10_000)\n",
    "agent.train()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5da0a65-fc50-4eeb-83ef-0eff088ba5c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#use the DQN\n",
    "env = make_env(gym.make(GAME, render_mode='human'), obs_type=\"rgb\")\n",
    "observation, info = env.reset()\n",
    "env.render()\n",
    "#show the steps the agent takes using the optimal policy table\n",
    "for i in range(10):\n",
    "    observation, info = env.reset()\n",
    "    terminated = truncated = False\n",
    "    rewards = 0\n",
    "    while not terminated and not truncated:\n",
    "        #find max policy\n",
    "        Q_values = agent.predict_q(np.expand_dims(observation, axis=0))\n",
    "        action = np.argmax(Q_values[0])\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        rewards += reward\n",
    "    print('Total reward is: '+str(rewards))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4798893-1c67-42c6-924e-02e72062bc89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addfd9c1-483d-4f85-b18f-c35dfb6ec45d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0203d703-54a8-4ec9-a4d1-460e62a3ffd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
